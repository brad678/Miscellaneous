---
title: "Clustering the Market Segments"
output: 
  html_document: 
    smart: no
---

### Project details: 

####Project: Clustering the Market Segments
####Data: Clustering the Market Segments.csv


####Description:
One way to gain an edge in segmented marketing to identify segments of people who share similar tastes, so that they can avoid targeting advertisements to people with no interest in the product being sold. For instance, an alcoholic beverage is likely to be difficult to sell to people who do not drink.
Given the text of Social Networking Service (SNS) pages of people, we can identify groups that share common interests such as sports, religion, or music. Clustering can automate the process of discovering the natural segments in this population.

####Objective:

- Cluster a number of products, with related products in one segment

####Guidelines:

- Explore and prepare the data
- Coding the missing values
- Imputation on missing values
- Training model on data
- Evaluation of model performance and visualization of segments.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, message=FALSE, warning=FALSE}
library(tidyr)
library(dplyr)
library(ggplot2)
library(caret)
library(pROC)
```

### Loading the data

```{r}

market <- read.csv('clustering the Market Segments.csv')
str(market)

```


### check missing values

```{r}
summary(market)

```

Can infer that there are missing values in:

- gender (1428)
- age (2400)
- abercrombie (1)
- die (1)
- death (1)
- drunk (1)
- drugs (1)

### Stats on missing values

```{r}
cat('\nThe number of missing values column wise are:',sum(is.na(market)))
cat('\nThe number of missing values row wise are:',nrow(market)-nrow(na.omit(market)))
library(VIM)
summary(aggr(market))

```

### Impute missing values
- "missForest"" is an implementation of random forest algorithm. It's a non parametric imputation method 

- Non-parametric method does not make explicit assumptions about functional form of f (any arbitary function). Instead, it tries to estimate f such that it can be as close to the data points without seeming impractical.

- It builds a random forest model for each variable. Then it uses the model to predict missing values in the variable with the help of observed values.

- It yield OOB (out of bag) imputation error estimate. Moreover, it provides high level of control on imputation process. It has options to return OOB separately (for each variable) instead of aggregating over the whole data matrix. This helps to look more closely as to how accurately the model has imputed values for each variable.

```{r}
#install.packages("missForest")
library(missForest)

#install.packages("doParallel")
library(doParallel)

registerDoParallel(cores = 2)


#impute missing values, we can apply the 'missForest' function while breaking the work down into equal numbers of 'variables' or 'forests' for each core to work on (here we break the number of variables).

market.imp <- missForest(market, parallelize = "variables")

```

### Checking the output from "missforest"

```{r}

#check imputed values
market1 <- market.imp$ximp
head(market1)

#check imputation error
market.imp$OOBerror

#check for missing values
summary(market1)

```

Can infer that:

- The normalized root mean squared error (NRMSE; for continuous variables) is 0.3%
- And the second is the proportion falsely classified (PFC; for categorical variables) is 25%


```{r}
glimpse(market1)
```

Can infer that:

- From the above date, we can make out that other than 4 variables(gradyear, gender,age, friends) the rest all indicate interests/taste of the people. 
- Thus, it would be logical to proceeded with these 36 variables for solving clustering problem.

### Normalising the data

```{r}

func_norm <- function(x)
{
  y <- (x-min(x))/(max(x)-min(x))
  return (y)
}


market2 <- market1[,5:40]

#market1_norm <- as.data.frame(scale(market2, center = TRUE, scale = TRUE))
market1_norm <- as.data.frame(sapply(market2,func_norm))


```


### Best K value (K means clustering)

```{r}

library(cluster)
k.max <- 8
data <- market1_norm
sil <- rep(0, k.max)
# Compute the average silhouette width for 
# k = 2 to k = 8
for(i in 2:k.max){
  km.res <- kmeans(data, centers = i, nstart = 25,iter.max = 20)
  ss <- silhouette(km.res$cluster, dist(data,"manhattan"))
  sil[i] <- mean(ss[, 3])
}
# Plot the  average silhouette width
plot(1:k.max, sil, type = "b", pch = 19, 
     frame = FALSE, xlab = "Number of clusters k",main='K means clustering')
abline(v = which.max(sil), lty = 2)

```

### Best K value (Hierarchical clustering)

```{r}

library(cluster)
k.max <- 8
data <- market1_norm
sil1 <- rep(0, k.max)
# Compute the average silhouette width for 
# k = 2 to k = 8
for(i in 2:k.max){
  hc.res <- hclust(dist(data),method = 'ward.D2')
  temp1 <- cutree(hc.res,i)
  ss <- silhouette(temp1, dist(data,"manhattan"))
  sil1[i] <- mean(ss[, 3])
}
# Plot the  average silhouette width
plot(1:k.max, sil1, type = "b", pch = 19, 
     frame = FALSE, xlab = "Number of clusters k",main='Heirarchical clustering')
abline(v = which.max(sil1), lty = 2)

```


### Best K value (PAM clustering)

```{r}

library(cluster)
k.max <- 8
data <- market1_norm
sil2 <- rep(0, k.max)
# Compute the average silhouette width for 
# k = 2 to k = 8
for(i in 2:k.max){
  pm.res <- pam(data, i)
  ss <- silhouette(pm.res$clustering, dist(data,"manhattan"))
  sil2[i] <- mean(ss[, 3])
}
# Plot the  average silhouette width
plot(1:k.max, sil2, type = "b", pch = 19, 
     frame = FALSE, xlab = "Number of clusters k",main='PAM clustering')
abline(v = which.max(sil2), lty = 2)

```

### Avg. silhouette width of various models

```{r}
max(sil)  #k means (2 clusters)
max(sil1) # hierarchical (2 clusters)
max(sil2) # pam (2 clusters)

```

### Choosing the best model

```{r}
max(sil,sil1,sil2)
```

Can infer that:

- Thes best model is that of k means cluster with 2 clusters

### The best model

```{r}

data <- market1_norm

#hc.res <- hclust(dist(data),method = 'ward.D2')
km.res <- kmeans(data, centers = 2, nstart = 25,iter.max = 20)

#grouping into 2 clusters
groups.2 <- km.res$cluster

#number of records in each cluster
table(groups.2)

```

### Plotting silhouette values

```{r}
ss <- silhouette(km.res$cluster, dist(data,"manhattan"))
ggplot(market1_norm,aes(as.factor(km.res$cluster),ss[,3],fill=as.factor(km.res$cluster)))+geom_violin()+coord_flip()+labs(y='Silhouette Value',x='Cluster',main='Silhouette Values for various clusters')

```


### Plotting (k means clustering)

```{r}

#2D representation of the Segmentation

library(cluster)
clusplot(market1_norm, groups.2, color=TRUE, shade=TRUE, labels=2, lines=0, main= 'Marketting segments')



#The cluster characterization using pie charts

pie(colSums(market1_norm[which(groups.2==1),]),cex=0.5,main='Cluster 1')
pie(colSums(market1_norm[which(groups.2==2),]),cex=0.5,main='Cluster 2')

```








