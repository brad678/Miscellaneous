---
title: "Loan Prediction"
output: 
  html_document: 
    smart: no
---

### Project details: 

####Domain: Banking and Finance
####Project: Loan Prediction
####Data: loan.zip

####Description:
This data corresponds to a set of financial transactions associated with individuals. You are provided with over one thousand observations (test + train) and nearly 13 features. Each observation is independent from the previous.

####Variable Description:

Loan_ID (Unique Loan ID)                                        
Gender (Male/ Female)                                  
Married (Applicant married (Y/N))                                                       
Dependents (Number of dependents)                            
Education (Applicant Education (Graduate/ Under Graduate))                    
Self_Employed (Self-employed (Y/N))                   
ApplicantIncome (Applicant income)                                                        
CoapplicantIncome (Coapplicant income)                                                    
LoanAmount (Loan amount in thousands)                                                    
Loan_Amount_Term (Term of loan in months)                                                        
Credit_History (Credit history meets guidelines. 1-good, 0-not good)    
Property_Area (Urban/ Semi Urban/ Rural)                                                         
Loan_Status (Loan approved (Y/N))                                                                                                                                                                                         


####Objective:

This project asks you to determine whether a loan will get approved or not. Also, try to find good insights with a financial management perspective.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, message=FALSE, warning=FALSE}
library(tidyr)
library(dplyr)
library(ggplot2)
library(caret)
```

### Loading the data

```{r}

loan_train <- read.csv('train_u6lujuX_CVtuZ9i.csv',na.strings = c('NA',''))
summary(loan_train)
glimpse(loan_train)

#removing loan_ID
loan_train <- loan_train[-1]

```

Can find the missing values in:

- gender (13)         
- married (3)           
- dependents (15)                   
- self-employed (32)            
- loan amount (22)            
- loan amount term (14)           
- credit history (50)         


### Finding columns and rows with missing values using function
```{r}
miss <- function(x) {
 cat('\nThe columns having missing values are:')
 for(i in 1:ncol(x)) {
  if(length(x[i][is.na(x[i])]) > 0) {
    cat('\n',names(x[i]),':',length(x[i][is.na(x[i])]))
  }
 } 
  cat('\n\nThe number of rows with missing values are:',nrow(x)-nrow(na.omit(x)))
  cat('\nThe proportion of missing values is:',signif((nrow(x)-nrow(na.omit(x)))/nrow(x),2))
}

miss(loan_train)
```

### Use kNN imputaion to handle missing values (k is choosen as square root of obs.)
### For numerical data, weighted average is used. The data points that are near are weighted more than far away data points.
### For Categorical data, mode is used

```{r, message=FALSE, warning=FALSE}
library(VIM)     #for kNN
library(laeken)  #for weightedMean

loan_train1 <- kNN(loan_train,variable=c('Gender','Married','Dependents','Self_Employed','LoanAmount','Loan_Amount_Term','Credit_History'),k=23,numFun = weightedMean,weightDist=TRUE)

summary(loan_train1)
glimpse(loan_train1)


```


### Remove the extra columns created by kNN imputation. 
### Validate any missing values are present


```{r}
loan_train1 <- loan_train1[-c(13:19)]
miss(loan_train1)

```

### Data Visualization

```{r}

ggplot(data=loan_train1,aes(Loan_Status,LoanAmount))+geom_boxplot()+labs(title='Loan Status vs Loan Amount')

ggplot(data=loan_train1,aes(Loan_Status,ApplicantIncome))+geom_boxplot()+labs(title='Loan Status vs Applicant Income')

ggplot(data=loan_train1,aes(LoanAmount,ApplicantIncome,color=Loan_Status))+geom_point()+labs(title='Loan Amount vs Applicant Income - for various Loan status')

ggplot(data=loan_train1,aes(LoanAmount,ApplicantIncome))+geom_point()+facet_wrap(~Loan_Status)+labs(title='Loan Amount vs Applicant Income - for various Loan status')

#checking if some relation between self employment and loan status
tab <- table(loan_status=loan_train1$Loan_Status,self_employed=loan_train1$Self_Employed)
cat('\nThe proportions of self employment - for various loan status:\n')
prop.table(tab,2)

#checking if some relation between Credit history and loan status
tab <- table(loan_status=loan_train1$Loan_Status,credit_history=loan_train1$Credit_History)
cat('\nThe proportions of credit history - for various loan status:\n')
prop.table(tab,2)
 

#checking if some relation between Coapplicant income and loan status
tab <- table(loan_status=loan_train1$Loan_Status,coapp_inc_0=loan_train1$CoapplicantIncome==0)
cat('\nThe proportions for coapplicant income zero - for various loan status:\n')
prop.table(tab,2)
 

#checking if some relation between education and loan status
tab <- table(loan_status=loan_train1$Loan_Status,education=loan_train1$Education)
cat('\nThe proportions of education - for various loan status:\n')
prop.table(tab,2)


#checking if some relation between marital status and loan status
tab <- table(loan_status=loan_train1$Loan_Status,married=loan_train1$Married)
cat('\nThe proportions of marital status - for various loan status:\n')
prop.table(tab,2)

#checking if some relation between number of dependents and loan status
tab <- table(loan_status=loan_train1$Loan_Status,num_of_dependents=loan_train1$Dependents)
cat('\nThe proportions of number of dependents - for various loan status:\n')
prop.table(tab,2)


#checking if some relation between property area and loan status
tab <- table(loan_status=loan_train1$Loan_Status,prop_area=loan_train1$Property_Area)
cat('\nThe proportions of property area - for various loan status:\n')
prop.table(tab,2)

```

Can infer that:

- The loans that got approved are the ones that took less loan amount compared to that of rejected.
- Applicant income doesn't seem different for approved/rejected loans
- As applicant income increases, loan amount increases. The rejected cases are mostly the ones with more loan amount and lesser applicant income (though there are lots of outliers)
- There seems no relation between self employment and loan status (approved cases for both self employed and non-self employed is same - 68%)
- The approved cases for credit_history "1" customers is significantly high(79%) than credit_history "0" customers(31%)
- The approval rate for cases where there is coaaplicant income is higher(71%) than cases where there is no coapplicant income(64%).
- The approval rate for cases where the applicant is married is higher(71%) than cases where there the applicant is not married(62%).
- The approval rate for cases where the number of dependents is <3 is greater than cases where there the number of dependents is 3+ 
- The approval rate for urban and semiurban(65% and 76%) properties is greater than rural properties(61%) 



### Splitting the data into train and validation set (70-30)

```{r}

set.seed(10)
loan_idx <- sample(1:nrow(loan_train1),0.7*nrow(loan_train1))
app_train <- loan_train1[loan_idx,]
app_val <- loan_train1[-loan_idx,]

```

### Logistic regression (Training the model)

```{r}

#Below is the model with significant variables after trying multiple combinations

logit.modelt <- glm(Loan_Status ~ Credit_History+Property_Area+LoanAmount+Married, family = 'binomial', data = app_train)


summary(logit.modelt)

```

Can infer below:

-  Credit_History, Property_Area, LoanAmount and Married are significant to the logistic regression model



### Logistic regression (Validation)
### Since the number of observations are less, it is better to use K-fold cross validation for correct assesment of accuracy/error in the model

```{r}

k = 10 #Folds
data <- loan_train1

# sample from 1 to k, nrow times (the number of observations in the data)
set.seed(11)
data$id <- sample(1:k, nrow(data), replace = TRUE)
table(data$id)

# prediction data frame that we add to with each iteration over the folds

prediction <- data.frame()


for (i in 1:k)
  {
  # select rows with id i to create test set
  # remove rows with id i from dataframe to create training set
  testset <- data %>% filter(id==i)
  trainingset <- setdiff(data,testset)
  
  # run the model
 
  mymodel <- glm(Loan_Status ~ Credit_History+Property_Area+LoanAmount+Married, family = 'binomial', data = trainingset)
  
  #mymodel <- glm(Loan_Status ~ Credit_History, family = 'binomial', data = trainingset)
  
  
  # predict Loan Status
  
  temp.probs <- predict(mymodel, testset,type='response')
  temp.pred <- rep('N',length(testset$Loan_Status))
  temp.pred[temp.probs>0.5]='Y'
  c1 <- confusionMatrix(table(predicted=temp.pred,actual=testset$Loan_Status),positive = 'Y')
  
  temp1 <- cbind(Accuracy=c1$overall[['Accuracy']],Sensitivity=c1$byClass[['Sensitivity']],Specificity=c1$byClass[['Specificity']],Precision=c1$byClass[['Precision']])
  
  # append this iteration's accuracy from confusion matrix to the end of data frame
  prediction <- rbind(prediction, temp1 ) 
  

}

# Use Mean Accuracy as Evalution 

k_evaluate <- sapply(prediction[,1:4],mean); k_evaluate


```

Can infer that:

- Accuracy of Logistic regression model is 0.7696

### Decision Tree  
### Since the number of observations are less, it is better to use K-fold cross validation for correct assesment of accuracy/error in the model

```{r}

k = 10 #Folds

table(data$id)  #data is split into 10 folds already above

# prediction data frame that we add to with each iteration over the folds

prediction <- data.frame()


for (i in 1:k)
  {
  # select rows with id i to create test set
  # remove rows with id i from dataframe to create training set
  testset <- data %>% filter(id==i)
  trainingset <- setdiff(data,testset)
  
  # run a decision tree model
  mymodel <- tree(Loan_Status ~ .,data=trainingset)
  #mymodel <- rpart(Loan_Status ~ .,data=trainingset)
  #mymodel_p <- prune(mymodel, cp=0.01) 
  
  
  # predict Loan Status
  
  temp <- predict(mymodel, testset,type='class')
 
  c1 <- confusionMatrix(table(predicted=temp,actual=testset$Loan_Status),positive = 'Y')
  
  temp1 <- cbind(Accuracy=c1$overall[['Accuracy']],Sensitivity=c1$byClass[['Sensitivity']],Specificity=c1$byClass[['Specificity']],Precision=c1$byClass[['Precision']])
  
  # append this iteration's accuracy from confusion matrix to the end of data frame
  prediction <- rbind(prediction, temp1 ) 
  

}

# Use Mean Accuracy as Evalution 

k_evaluate <- sapply(prediction[,1:4],mean); k_evaluate


```

Can infer that:

- The accuracy of decision tree is same as that of logistic regression.

### Random Forest  
### Since the number of observations are less, it is better to use K-fold cross validation for correct assesment of accuracy/error in the model

```{r}

library(randomForest)
k = 10 #Folds

table(data$id)  #data is split into 10 folds already above

# prediction data frame that we add to with each iteration over the folds

prediction <- data.frame()


for (i in 1:k)
  {
  # select rows with id i to create test set
  # remove rows with id i from dataframe to create training set
  testset <- data %>% filter(id==i)
  trainingset <- setdiff(data,testset)
  
  # run a randomForest model
  set.seed(14)
  mymodel <- randomForest(Loan_Status ~ .,data=trainingset,ntree=700,mtry=5)
  
  
  # predict Loan Status
  
  temp <- predict(mymodel, testset,type='class')
 
  c1 <- confusionMatrix(table(predicted=temp,actual=testset$Loan_Status),positive = 'Y')
  
  temp1 <- cbind(Accuracy=c1$overall[['Accuracy']],Sensitivity=c1$byClass[['Sensitivity']],Specificity=c1$byClass[['Specificity']],Precision=c1$byClass[['Precision']])
  
  # append this iteration's accuracy from confusion matrix to the end of data frame
  prediction <- rbind(prediction, temp1 ) 
  

}

# Use Mean Accuracy as Evalution 

k_evaluate <- sapply(prediction[,1:4],mean); k_evaluate


```

Can infer that:

- The accuracy of random forest is pretty similar(although slightly less) to that of logistic regression and decision tree
- Reason for that might be: 
    If the predictions of the trees are stable, all submodels in the ensemble return the same         prediction and then the prediction of the random forest is just the same as the prediction of     each single tree.
    So then the overall performance be the same 
    
    
    