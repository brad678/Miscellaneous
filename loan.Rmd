---
title: "Loan Prediction"
output: 
  html_document: 
    smart: no
---

### Project details: 

####Domain: Banking and Finance
####Project: Loan Prediction
####Data: loan.zip

####Description:
This data corresponds to a set of financial transactions associated with individuals. You are provided with over one thousand observations (test + train) and nearly 13 features. Each observation is independent from the previous.

####Variable Description:

Loan_ID (Unique Loan ID)                                        
Gender (Male/ Female)                                  
Married (Applicant married (Y/N))                                                       
Dependents (Number of dependents)                            
Education (Applicant Education (Graduate/ Under Graduate))                    
Self_Employed (Self-employed (Y/N))                   
ApplicantIncome (Applicant income)                                                        
CoapplicantIncome (Coapplicant income)                                                    
LoanAmount (Loan amount in thousands)                                                    
Loan_Amount_Term (Term of loan in months)                                                        
Credit_History (Credit history meets guidelines. 1-good, 0-not good)    
Property_Area (Urban/ Semi Urban/ Rural)                                                         
Loan_Status (Loan approved (Y/N))                                                                                                                                                                                         


####Objective:

This project asks you to determine whether a loan will get approved or not. Also, try to find good insights with a financial management perspective.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, message=FALSE, warning=FALSE}
library(tidyr)
library(dplyr)
library(ggplot2)
library(caret)
```

### Loading the data

```{r}

loan_train <- read.csv('train_u6lujuX_CVtuZ9i.csv',na.strings = c('NA',''))
summary(loan_train)
glimpse(loan_train)

#removing loan_ID
loan_train <- loan_train[-1]

```

Can find the missing values in:

- gender (13)         
- married (3)           
- dependents (15)                   
- self-employed (32)            
- loan amount (22)            
- loan amount term (14)           
- credit history (50)         


### Finding columns and rows with missing values using function
```{r}
miss <- function(x) {
 cat('\nThe columns having missing values are:')
 for(i in 1:ncol(x)) {
  if(length(x[i][is.na(x[i])]) > 0) {
    cat('\n',names(x[i]),':',length(x[i][is.na(x[i])]))
  }
 } 
  cat('\n\nThe number of rows with missing values are:',nrow(x)-nrow(na.omit(x)))
  cat('\nThe proportion of missing values is:',signif((nrow(x)-nrow(na.omit(x)))/nrow(x),2))
}

miss(loan_train)
```

### Use kNN imputaion to handle missing values (k is choosen as square root of obs.)
### For numerical data, weighted average is used. The data points that are near are weighted more than far away data points.
### For Categorical data, mode is used

```{r, message=FALSE, warning=FALSE}
library(VIM)     #for kNN
library(laeken)  #for weightedMean

loan_train1 <- kNN(loan_train,variable=c('Gender','Married','Dependents','Self_Employed','LoanAmount','Loan_Amount_Term','Credit_History'),k=23,numFun = weightedMean,weightDist=TRUE)

summary(loan_train1)
glimpse(loan_train1)


```


### Remove the extra columns created by kNN imputation. 
### Validate any missing values are present


```{r}
loan_train1 <- loan_train1[-c(13:19)]
miss(loan_train1)

```

### Data Visualization

```{r}

ggplot(data=loan_train1,aes(Loan_Status,LoanAmount))+geom_boxplot()+labs(title='Loan Status vs Loan Amount')

ggplot(data=loan_train1,aes(Loan_Status,ApplicantIncome))+geom_boxplot()+labs(title='Loan Status vs Applicant Income')

ggplot(data=loan_train1,aes(LoanAmount,ApplicantIncome,color=Loan_Status))+geom_point()+labs(title='Loan Amount vs Applicant Income - for various Loan status')

ggplot(data=loan_train1,aes(LoanAmount,ApplicantIncome))+geom_point()+facet_wrap(~Loan_Status)+labs(title='Loan Amount vs Applicant Income - for various Loan status')

#checking if some relation between self employment and loan status
tab <- table(loan_status=loan_train1$Loan_Status,self_employed=loan_train1$Self_Employed)
cat('\nThe proportions of self employment - for various loan status:\n')
prop.table(tab,2)

#checking if some relation between Credit history and loan status
tab <- table(loan_status=loan_train1$Loan_Status,credit_history=loan_train1$Credit_History)
cat('\nThe proportions of credit history - for various loan status:\n')
prop.table(tab,2)
 

#checking if some relation between Coapplicant income and loan status
tab <- table(loan_status=loan_train1$Loan_Status,coapp_inc_0=loan_train1$CoapplicantIncome==0)
cat('\nThe proportions for coapplicant income zero - for various loan status:\n')
prop.table(tab,2)
 

#checking if some relation between education and loan status
tab <- table(loan_status=loan_train1$Loan_Status,education=loan_train1$Education)
cat('\nThe proportions of education - for various loan status:\n')
prop.table(tab,2)


#checking if some relation between marital status and loan status
tab <- table(loan_status=loan_train1$Loan_Status,married=loan_train1$Married)
cat('\nThe proportions of marital status - for various loan status:\n')
prop.table(tab,2)

#checking if some relation between number of dependents and loan status
tab <- table(loan_status=loan_train1$Loan_Status,num_of_dependents=loan_train1$Dependents)
cat('\nThe proportions of number of dependents - for various loan status:\n')
prop.table(tab,2)


#checking if some relation between property area and loan status
tab <- table(loan_status=loan_train1$Loan_Status,prop_area=loan_train1$Property_Area)
cat('\nThe proportions of property area - for various loan status:\n')
prop.table(tab,2)

```

Can infer that:

- The loans that got approved are the ones that took less loan amount compared to that of rejected.
- Applicant income doesn't seem different for approved/rejected loans
- As applicant income increases, loan amount increases. The rejected cases are mostly the ones with more loan amount and lesser applicant income (though there are lots of outliers)
- There seems no relation between self employment and loan status (approved cases for both self employed and non-self employed is same - 68%)
- The approved cases for credit_history "1" customers is significantly high(79%) than credit_history "0" customers(31%)
- The approval rate for cases where there is coaaplicant income is higher(71%) than cases where there is no coapplicant income(64%).
- The approval rate for cases where the applicant is married is higher(71%) than cases where there the applicant is not married(62%).
- The approval rate for cases where the number of dependents is <3 is greater than cases where there the number of dependents is 3+ 
- The approval rate for urban and semiurban(65% and 76%) properties is greater than rural properties(61%) 



### Splitting the data into train and validation set (70-30)

```{r}

set.seed(10)
loan_idx <- sample(1:nrow(loan_train1),0.7*nrow(loan_train1))
app_train <- loan_train1[loan_idx,]
app_val <- loan_train1[-loan_idx,]

```

### Logistic regression (Training the model)

```{r}

#Below is the model with significant variables after trying multiple combinations

logit.modelt <- glm(Loan_Status ~ Credit_History+Property_Area+LoanAmount+Married, family = 'binomial', data = app_train)


summary(logit.modelt)

```

Can infer below:

-  Credit_History, Property_Area, LoanAmount and Married are significant to the logistic regression model



### Logistic regression (Validation)
### Since the number of observations are less, it is better to use K-fold cross validation for correct assesment of accuracy/error in the model

```{r}

k = 10 #Folds
data <- loan_train1

# sample from 1 to k, nrow times (the number of observations in the data)
set.seed(11)
data$id <- sample(1:k, nrow(data), replace = TRUE)
table(data$id)

# prediction data frame that we add to with each iteration over the folds

prediction <- data.frame()


for (i in 1:k)
  {
  # select rows with id i to create test set
  # remove rows with id i from dataframe to create training set
  testset <- data %>% filter(id==i)
  trainingset <- setdiff(data,testset)
  
  # run the model
 
  mymodel <- glm(Loan_Status ~ Credit_History+Property_Area+LoanAmount+Married, family = 'binomial', data = trainingset)
  
  #mymodel <- glm(Loan_Status ~ Credit_History, family = 'binomial', data = trainingset)
  
  
  # predict Loan Status
  
  temp.probs <- predict(mymodel, testset,type='response')
  temp.pred <- rep('N',length(testset$Loan_Status))
  temp.pred[temp.probs>0.5]='Y'
  c1 <- confusionMatrix(table(predicted=temp.pred,actual=testset$Loan_Status),positive = 'Y')
  
  temp1 <- cbind(Accuracy=c1$overall[['Accuracy']],Sensitivity=c1$byClass[['Sensitivity']],Specificity=c1$byClass[['Specificity']],Precision=c1$byClass[['Precision']])
  
  # append this iteration's accuracy from confusion matrix to the end of data frame
  prediction <- rbind(prediction, temp1 ) 
  

}

# Use Mean Accuracy as Evalution 

k_evaluate <- sapply(prediction[,1:4],mean); k_evaluate


```

Can infer that:

- Accuracy of Logistic regression model is 0.7696

### Decision Tree  
### Since the number of observations are less, it is better to use K-fold cross validation for correct assesment of accuracy/error in the model

```{r}

library(tree)
k = 10 #Folds

table(data$id)  #data is split into 10 folds already above

# prediction data frame that we add to with each iteration over the folds

prediction <- data.frame()


for (i in 1:k)
  {
  # select rows with id i to create test set
  # remove rows with id i from dataframe to create training set
  testset <- data %>% filter(id==i)
  trainingset <- setdiff(data,testset)
  
  # run a decision tree model
  mymodel <- tree(Loan_Status ~ .,data=trainingset)
  #mymodel <- rpart(Loan_Status ~ .,data=trainingset)
  #mymodel_p <- prune(mymodel, cp=0.01) 
  
  
  # predict Loan Status
  
  temp <- predict(mymodel, testset,type='class')
 
  c1 <- confusionMatrix(table(predicted=temp,actual=testset$Loan_Status),positive = 'Y')
  
  temp1 <- cbind(Accuracy=c1$overall[['Accuracy']],Sensitivity=c1$byClass[['Sensitivity']],Specificity=c1$byClass[['Specificity']],Precision=c1$byClass[['Precision']])
  
  # append this iteration's accuracy from confusion matrix to the end of data frame
  prediction <- rbind(prediction, temp1 ) 
  

}

# Use Mean Accuracy as Evalution 

k_evaluate <- sapply(prediction[,1:4],mean); k_evaluate


```

Can infer that:

- The accuracy of decision tree is same as that of logistic regression.

### Random Forest  
### Since the number of observations are less, it is better to use K-fold cross validation for correct assesment of accuracy/error in the model

```{r}

library(randomForest)
k = 10 #Folds

table(data$id)  #data is split into 10 folds already above

# prediction data frame that we add to with each iteration over the folds

prediction <- data.frame()


for (i in 1:k)
  {
  # select rows with id i to create test set
  # remove rows with id i from dataframe to create training set
  testset <- data %>% filter(id==i)
  trainingset <- setdiff(data,testset)
  
  # run a randomForest model
  set.seed(14)
  mymodel <- randomForest(Loan_Status ~ .,data=trainingset,ntree=700,mtry=5)
  
  
  # predict Loan Status
  
  temp <- predict(mymodel, testset,type='class')
 
  c1 <- confusionMatrix(table(predicted=temp,actual=testset$Loan_Status),positive = 'Y')
  
  temp1 <- cbind(Accuracy=c1$overall[['Accuracy']],Sensitivity=c1$byClass[['Sensitivity']],Specificity=c1$byClass[['Specificity']],Precision=c1$byClass[['Precision']])
  
  # append this iteration's accuracy from confusion matrix to the end of data frame
  prediction <- rbind(prediction, temp1 ) 
  

}

# Use Mean Accuracy as Evalution 

k_evaluate <- sapply(prediction[,1:4],mean); k_evaluate


```

Can infer that:

- The accuracy of random forest is pretty similar(although slightly less) to that of logistic regression and decision tree
- Reason for that might be: 
    If the predictions of the trees are stable, all submodels in the ensemble return the same prediction and then the prediction of the random forest is just the same as the prediction of each single tree.
    So then the overall performance be the same 
 
 
    
    
### KNN 
### Since the number of observations are less, it is better to use K-fold cross validation for correct assesment of accuracy/error in the model


```{r}
data1 <- data

```


```{r}

library(class)
k = 10 #Folds


#converting non-numeric data to numeric
data[,c(1:5,11,12)] <- sapply(data[,c(1:5,11,12)], function(x) as.numeric(x))

#converting loan status back from numeric to factor 
data[,12] <- as.factor(data[,12])

#normalise appl income,co-appl income,loan amount, loan amount term
data[,6:9] <- scale(data[,6:9],center=TRUE,scale=TRUE)

table(data$id)  #data is split into 10 folds already above

```

 
### Function that does KNN classification for a value of 'k'

```{r}


knn_func <- function(j) {
 
  # prediction data frame that we add to with each iteration over the folds
  prediction <- data.frame()


for (i in 1:k)
  {
  # select rows with id i to create test set
  # remove rows with id i from dataframe to create training set
  testset <- data %>% filter(id==i)
  trainingset <- setdiff(data,testset)
  
  # run a knn model
  
  knn_pred <<- knn(train=trainingset,test=testset,cl=trainingset[,"Loan_Status"],k=j)
   
  
  
  # predict Loan Status
  
  c1 <- confusionMatrix(table(predicted=knn_pred,actual=testset$Loan_Status),positive = '2')
  
  temp1 <- cbind(Accuracy=c1$overall[['Accuracy']],Sensitivity=c1$byClass[['Sensitivity']],Specificity=c1$byClass[['Specificity']],Precision=c1$byClass[['Precision']])
  
  # append this iteration's accuracy from confusion matrix to the end of data frame
  prediction <- rbind(prediction, temp1 ) 
  

}

# Use Mean Accuracy as Evalution 

k_evaluate <- sapply(prediction[,1:4],mean); k_evaluate

return(k_evaluate)

}


```

### Running the KNN model for varying values of 'k'

```{r}

knn_eval <- NULL
knn_array <- data.frame()
for (j in 1:30)
{
  knn_eval <- knn_func(j)
  knn_array <- rbind(knn_array,t(as.data.frame(knn_eval)))
  
}
  

knn_array

```


Can infer that:

- The accuracy of KNN is better than that of logistic regresion, decision tree and random forest
- As K value is increasing the accuracy is decreasing.
- Choice of k is very critical - A small value of k means that noise will have a higher influence on the result. A large value make it computationally expensive and kinda defeats the basic philosophy behind KNN 
- Let's choose K as an optimal value K = sqrt(N)/2, where N stands for the number of samples in your training dataset. K choosen here is 11.

```{r}

knn_array[11,]

```


### Naive Bayes 
### Since the number of observations are less, it is better to use K-fold cross validation for correct assesment of accuracy/error in the model

```{r}

data <- data1

```



```{r}

library(e1071)
k = 10 #Folds

table(data$id)  #data is split into 10 folds already above

# prediction data frame that we add to with each iteration over the folds

prediction <- data.frame()


for (i in 1:k)
  {
  # select rows with id i to create test set
  # remove rows with id i from dataframe to create training set
  testset <- data %>% filter(id==i)
  trainingset <- setdiff(data,testset)
  
  # run a Naive Bayes model
  

  
  #mymodel <- naiveBayes(trainingset,trainingset$Loan_Status)
  mymodel <- naiveBayes(Loan_Status~.,trainingset)

  # predict Loan Status
  
  temp <- predict(mymodel, testset,type='class')
  
  
  c1 <- confusionMatrix(table(temp,testset$Loan_Status),positive = 'Y')
   
  
  temp1 <- cbind(Accuracy=c1$overall[['Accuracy']],Sensitivity=c1$byClass[['Sensitivity']],Specificity=c1$byClass[['Specificity']],Precision=c1$byClass[['Precision']])
  
  # append this iteration's accuracy from confusion matrix to the end of data frame
  prediction <- rbind(prediction, temp1 ) 
  

}

# Use Mean Accuracy as Evalution 

k_evaluate <- sapply(prediction[,1:4],mean); k_evaluate


```


Can infer that:

- This model has given accuracy of 0.7554


### SVM 
### Since the number of observations are less, it is better to use K-fold cross validation for correct assesment of accuracy/error in the model



```{r}

library(e1071)
k = 10 #Folds

table(data$id)  #data is split into 10 folds already above

# prediction data frame that we add to with each iteration over the folds

prediction <- data.frame()


for (i in 1:k)
  {
  # select rows with id i to create test set
  # remove rows with id i from dataframe to create training set
  testset <- data %>% filter(id==i)
  trainingset <- setdiff(data,testset)
  
  # run a SVM model
  
  mymodel <- svm(Loan_Status~.,data=trainingset,kernel='linear',cost=0.01,scale=T)
  

  # predict Loan Status
  
  temp <- predict(mymodel, testset,type='class')
  
  
  c1 <- confusionMatrix(table(temp,testset$Loan_Status),positive = 'Y')
   
  
  temp1 <- cbind(Accuracy=c1$overall[['Accuracy']],Sensitivity=c1$byClass[['Sensitivity']],Specificity=c1$byClass[['Specificity']],Precision=c1$byClass[['Precision']])
  
  # append this iteration's accuracy from confusion matrix to the end of data frame
  prediction <- rbind(prediction, temp1 ) 
  

}

# Use Mean Accuracy as Evalution 

k_evaluate <- sapply(prediction[,1:4],mean); k_evaluate


```

Can infer that:

- The accuracy of svm model is 0.7696



### The accuracies of all models are as follows:

logistic regression: 0.7696929
decision tree      : 0.7696929
random forest      : 0.7569171
KNN                : 0.8102985 (11 neighbors)
Naive Bayes        : 0.7554259
SVM                : 0.7696929





 
### Function that does KNN classification for a value of 'k'

```{r}


knn_funcw <- function(j) {
 
  # prediction data frame that we add to with each iteration over the folds
  prediction <- data.frame()


for (i in 1:k)
  {
  # select rows with id i to create test set
  # remove rows with id i from dataframe to create training set
  testset <- data %>% filter(id==i)
  trainingset <- setdiff(data,testset)
  
  # run a kknn model
  
  kknn_pred <- kknn(Loan_Status~.,trainingset,testset,scale = TRUE, distance = 7, kernel = "triangular",k=j)
   
  
  
  # predict Loan Status
  
  c1 <- confusionMatrix(table(predicted=fitted(kknn_pred),actual=testset$Loan_Status),positive = 'Y')
  
  temp1 <- cbind(Accuracy=c1$overall[['Accuracy']],Sensitivity=c1$byClass[['Sensitivity']],Specificity=c1$byClass[['Specificity']],Precision=c1$byClass[['Precision']])
  
  # append this iteration's accuracy from confusion matrix to the end of data frame
  prediction <- rbind(prediction, temp1 ) 
  

}

# Use Mean Accuracy as Evalution 

k_evaluate <- sapply(prediction[,1:4],mean); k_evaluate

return(k_evaluate)

}


```

### Running the KNN model for varying values of 'k'

```{r}

library(kknn)
k = 10 #Folds
knn_eval <- NULL
knn_array <- data.frame()


knn_eval <- knn_funcw(14)
knn_array <- rbind(knn_array,t(as.data.frame(knn_eval)))
  


knn_array

```


```{r}
#Spliting training set into two parts based on outcome: 70% and 30%
set.seed(119)
index <- createDataPartition(loan_train1$Loan_Status, p=0.70, list=FALSE)
trainSet <- loan_train1[ index,]
testSet <- loan_train1[-index,]

```




```{r}
#Defining the training controls for multiple models
fitControl <- trainControl(
  method = "cv",
  number = 10,
  savePredictions = 'final',
  classProbs = T)

#Defining the predictors and outcome
predictors<-c("Credit_History", "LoanAmount", "Loan_Amount_Term", "ApplicantIncome",
  "CoapplicantIncome","Property_Area","Gender","Married","Dependents","Education","Self_Employed")

outcomeName<-'Loan_Status'

```


 
 
 
```{r}

#Training the random forest model
model_rf<-train(trainSet[,predictors],trainSet[,outcomeName],method='rf',trControl=fitControl)

#Predicting using random forest model
predict_model_rf<-predict(model_rf,testSet[,predictors])

#Checking the accuracy of the random forest model
confusionMatrix(predict_model_rf,testSet[,outcomeName])


```
