---
title: "Loan Prediction"
output: 
  html_document: 
    smart: no
---

### Project details: 

####Domain: Banking and Finance
####Project: Loan Prediction
####Data: loan.zip

####Description:
This data corresponds to a set of financial transactions associated with individuals. You are provided with over one thousand observations (test + train) and nearly 13 features. Each observation is independent from the previous.

####Variable Description:

Loan_ID (Unique Loan ID)                                        
Gender (Male/ Female)                                  
Married (Applicant married (Y/N))                                                       
Dependents (Number of dependents)                            
Education (Applicant Education (Graduate/ Under Graduate))                    
Self_Employed (Self-employed (Y/N))                   
ApplicantIncome (Applicant income)                                                        
CoapplicantIncome (Coapplicant income)                                                    
LoanAmount (Loan amount in thousands)                                                    
Loan_Amount_Term (Term of loan in months)                                                        
Credit_History (Credit history meets guidelines. 1-good, 0-not good)    
Property_Area (Urban/ Semi Urban/ Rural)                                                         
Loan_Status (Loan approved (Y/N))                                                                                                                                                                                         


####Objective:

This project asks you to determine whether a loan will get approved or not. Also, try to find good insights with a financial management perspective.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, message=FALSE, warning=FALSE}
library(tidyr)
library(dplyr)
library(ggplot2)
library(caret)
```

### Loading the data

```{r}

loan_train <- read.csv('train_u6lujuX_CVtuZ9i.csv',na.strings = c('NA',''))
summary(loan_train)
glimpse(loan_train)

#removing loan_ID
loan_train <- loan_train[-1]
#making the variables as factor
loan_train$Credit_History <- as.factor(loan_train$Credit_History)
loan_train$Loan_Amount_Term <- as.factor(loan_train$Loan_Amount_Term)

```

Can find the missing values in:

- gender (13)         
- married (3)           
- dependents (15)                   
- self-employed (32)            
- loan amount (22)            
- loan amount term (14)           
- credit history (50)         


### Finding columns and rows with missing values using function
```{r}
miss <- function(x) {
 cat('\nThe columns having missing values are:')
 for(i in 1:ncol(x)) {
  if(length(x[i][is.na(x[i])]) > 0) {
    cat('\n',names(x[i]),':',length(x[i][is.na(x[i])]))
  }
 } 
  cat('\n\nThe number of rows with missing values are:',nrow(x)-nrow(na.omit(x)))
  cat('\nThe proportion of missing values is:',signif((nrow(x)-nrow(na.omit(x)))/nrow(x),2))
}

miss(loan_train)
```

### Use kNN imputaion to handle missing values (k is choosen as square root of obs.)
### For numerical data, weighted average is used. The data points that are near are weighted more than far away data points.
### For Categorical data, mode is used

```{r, message=FALSE, warning=FALSE}
library(VIM)     #for kNN
library(laeken)  #for weightedMean

loan_train1 <- kNN(loan_train,variable=c('Gender','Married','Dependents','Self_Employed','LoanAmount','Loan_Amount_Term','Credit_History'),k=23,numFun = weightedMean,weightDist=TRUE)

summary(loan_train1)
glimpse(loan_train1)


```


### Remove the extra columns created by kNN imputation. 
### Validate any missing values are present


```{r}
loan_train1 <- loan_train1[-c(13:19)]
miss(loan_train1)

```

### Data Visualization (Univariate analysis)


```{r}

cols <- c('Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Credit_History', 'Property_Area', 'Loan_Status','Loan_Amount_Term')
for (i in 1:length(cols))
{
  pl <- ggplot(loan_train1,aes(x=loan_train1[,cols[i]]))+geom_bar()+labs(x=cols[i],title=cols[i])
  print(pl)
}


```

Can infer that:

- There are more male customers who applied for loan
- There are more married customers
- There are more number of customers with 0 dependents. 3+ dependents are very less
- There are more graduates who applied for loan
- There are less customers having occupation as self employed
- There are more customers with credit history of 1
- There are more customers who applied loan for semi-urban and urban areas
- There are more customers whose loans are approved
- There are many customers who applied for loan term as 360 months


### Data Visualization (Bivariate analysis)


```{r}

cols <- c('Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Credit_History', 'Property_Area', 'Loan_Amount_Term')
for (i in 1:length(cols))
{
  pl <- ggplot(loan_train1,aes(loan_train1$Loan_Status))+geom_bar()+facet_grid(~loan_train1[,cols[i]])+labs(x='Loan Status',title=cols[i])
  print(pl)
  
}


ggplot(data=loan_train1,aes(Loan_Status,LoanAmount))+geom_boxplot()+labs(title='Loan Status vs Loan Amount')

ggplot(data=loan_train1,aes(Loan_Status,ApplicantIncome))+geom_boxplot()+labs(title='Loan Status vs Applicant Income')

ggplot(data=loan_train1,aes(Loan_Status,CoapplicantIncome))+geom_boxplot()+labs(title='Loan Status vs CoApplicant Income')




```

Can infer that:



- More approvals for males (as proportion is high, otherwise not much difference)
- More approvals for married customers (as proportion is high, otherwise not much difference)
- More approvals for 0 dependents (as proportion is high, otherwise not much difference) 
- More approvals for graduates (as proportion is high, otherwise not much difference) 
- Less approvals for self-employed (as proportion is low, otherwise not much difference)  
- More approvals for credit history of 1 (as proportion is high, otherwise not much difference)  
- More approvals for semi-urban and urban areas (as proportion is high, otherwise not much difference)  
- More approvals for loan term as 360 months (as proportion is high, otherwise not much difference) 
- The loans that got approved are the ones that took less loan amount compared to that of rejected
- Applicant income doesn't seem different for approved/rejected loans
- Coapplicant income doesn't seem different for approved/rejected loans


### Data Visualization (Multivariate analysis)


```{r}

ggplot(data=loan_train1,aes(LoanAmount,ApplicantIncome,color=Loan_Status))+geom_point()+labs(title='Loan Amount vs Applicant Income - for various Loan status')

ggplot(data=loan_train1,aes(LoanAmount,ApplicantIncome))+geom_point()+facet_wrap(~Loan_Status)+labs(title='Loan Amount vs Applicant Income - for various Loan status')

```

Can infer that:

- As applicant income increases, loan amount increases. The rejected cases are mostly the ones with more loan amount and lesser applicant income (though there are lots of outliers)


### Spliting training set into two parts based on outcome: 75% and 25%

```{r}

set.seed(100)
index <- createDataPartition(loan_train1$Loan_Status, p=0.75, list=FALSE)
trainSet <- loan_train1[ index,]
testSet <- loan_train1[-index,]

```


### Defining the training controls for multiple models

```{r}
#using 10-fold cross validation
fitControl <- trainControl(
  method = "cv",
  number = 10,
  savePredictions = 'final',
  classProbs = T)

```

###Defining the predictors and outcome

```{r}

predictors<-c("Credit_History", "LoanAmount", "Loan_Amount_Term", "ApplicantIncome",
  "CoapplicantIncome","Property_Area","Gender","Married","Dependents","Education","Self_Employed")


outcomeName<-'Loan_Status'

```

### Initializing the table

```{r}
prediction <<- data.frame()

```

### Normalising the data

```{r}
trainSet1 <- trainSet

#converting non-numeric data to numeric
trainSet1[,c(1:5,11)] <- sapply(trainSet1[,c(1:5,11)], function(x) as.numeric(x))



#normalise appl income,co-appl income,loan amount, loan amount term
trainSet1[,c(6:9)] <- scale(trainSet1[c(6:9)],center=TRUE,scale=TRUE)
#trainSet1[,c(6:9)] <- sapply(trainSet1[,c(6:9)], function(x) ifelse(x==0,0,log(x)))


#loan_train1$FamilySize <- ifelse((loan_train1$CoapplicantIncome>0|loan_train1$Married=="Y"),numDependents+2,numDependents+1)

testSet1 <- testSet

#converting non-numeric data to numeric
testSet1[,c(1:5,11)] <- sapply(testSet1[,c(1:5,11)], function(x) as.numeric(x))



#normalise appl income,co-appl income,loan amount, loan amount term
testSet1[,c(6:9)] <- scale(testSet1[,c(6:9)],center=TRUE,scale=TRUE)
#testSet1[,c(6:9)] <- sapply(testSet1[,c(6:9)], function(x) ifelse(x==0,0,log(x)))

```


### Logistic regression

```{r}

#Training the logistic regression model

set.seed(101)
model_lr<-train(trainSet1[,predictors],trainSet1[,outcomeName],method='glm',trControl=fitControl)
model_lr

#Predicting using logistic regression model
predict_model_lr<-predict(model_lr,testSet1[,predictors])

#Checking the accuracy of the logistic regression model
c1 <- confusionMatrix(predict_model_lr,testSet1[,outcomeName],positive = 'Y')

temp1 <- cbind(Accuracy=c1$overall[['Accuracy']],Sensitivity=c1$byClass[['Sensitivity']],Specificity=c1$byClass[['Specificity']],Precision=c1$byClass[['Precision']])

prediction <<- prediction[-1,]
prediction <<- rbind(prediction, log_reg=temp1 )
prediction['log_reg',]

```
 
 Can infer that:
 
 -  Accuracy is 0.7647
 
```{r}

#Training the random forest model

set.seed(101)
model_rf<-train(trainSet1[,predictors],trainSet1[,outcomeName],method='rf',trControl=fitControl)
model_rf

#Predicting using random forest model
predict_model_rf<-predict(model_rf,testSet1[,predictors])

#Checking the accuracy of the random forest model
c1 <- confusionMatrix(predict_model_rf,testSet1[,outcomeName],positive = 'Y')

temp1 <- cbind(Accuracy=c1$overall[['Accuracy']],Sensitivity=c1$byClass[['Sensitivity']],Specificity=c1$byClass[['Specificity']],Precision=c1$byClass[['Precision']])

prediction <<- prediction[-2,]
prediction <<- rbind(prediction, random_forest=temp1 )
prediction['random_forest',]


```

Can infer that:

- The optimal value of mtry choosen is 2
- Accuracy is 0.7843



```{r}

#Training the knn model

set.seed(101)



model_knn<-train(trainSet1[,predictors],trainSet1[,outcomeName],method='knn',trControl=fitControl)
model_knn

#Predicting using knn model
predict_model_knn <- predict(model_knn,testSet1[,predictors])

#Checking the accuracy of the knn model
c1 <- confusionMatrix(predict_model_knn,testSet1[,outcomeName],positive = 'Y')


temp1 <- cbind(Accuracy=c1$overall[['Accuracy']],Sensitivity=c1$byClass[['Sensitivity']],Specificity=c1$byClass[['Specificity']],Precision=c1$byClass[['Precision']])

prediction <<- prediction[-3,]
prediction <<- rbind(prediction, k_nearest=temp1 )
prediction['k_nearest',]


```

Can infer that:

- The optimal number of neighbours choosen is 7
- Accuracy is 0.6993


```{r}

#Training the rpart model/decision tree

set.seed(101)
model_rpart<-train(trainSet1[,predictors],trainSet1[,outcomeName],method='rpart',trControl=fitControl)
model_rpart

#Predicting using rpart model
predict_model_rpart<-predict(model_rpart,testSet1[,predictors])

#Checking the accuracy of the rpart model
c1 <- confusionMatrix(predict_model_rpart,testSet1[,outcomeName],positive = 'Y')


temp1 <- cbind(Accuracy=c1$overall[['Accuracy']],Sensitivity=c1$byClass[['Sensitivity']],Specificity=c1$byClass[['Specificity']],Precision=c1$byClass[['Precision']])

prediction <<- prediction[-4,]
prediction <<- rbind(prediction, decision_tree=temp1 )
prediction['decision_tree',]


```

Can infer that:

- The optimal value of complexity paramter(CP) choosen is 0.01736111
- The complexity parameter (cp) is used to control the size of the decision tree and to select the optimal tree size. If the cost of adding another variable to the decision tree from the current node is above the value of cp, then tree building does not continue
- Accuracy is 0.7647



```{r}

#Training the naive bayes model

set.seed(101)
model_nb<-train(trainSet1[,predictors],trainSet1[,outcomeName],method='nb',trControl=fitControl)
model_nb

#Predicting using naive bayes model
predict_model_nb<-predict(model_nb,testSet1[,predictors])

#Checking the accuracy of the naive bayes model
c1 <- confusionMatrix(predict_model_nb,testSet1[,outcomeName],positive = 'Y')

temp1 <- cbind(Accuracy=c1$overall[['Accuracy']],Sensitivity=c1$byClass[['Sensitivity']],Specificity=c1$byClass[['Specificity']],Precision=c1$byClass[['Precision']])

prediction <<- prediction[-5,]
prediction <<- rbind(prediction, naive_bayes=temp1 )
prediction['naive_bayes',]


```


Can infer that:

- The optimal value of kernel is FALSE
- Accuracy is 0.7189


```{r}

#Training the svm model

set.seed(101)
model_svm <- train(trainSet1[,predictors],trainSet1[,outcomeName],method='svmLinear',trControl=fitControl)
model_svm

#Predicting using svm model
predict_model_svm<-predict(model_svm,testSet1[,predictors])

#Checking the accuracy of the svm model
c1 <- confusionMatrix(predict_model_svm,testSet1[,outcomeName],positive = 'Y')


temp1 <- cbind(Accuracy=c1$overall[['Accuracy']],Sensitivity=c1$byClass[['Sensitivity']],Specificity=c1$byClass[['Specificity']],Precision=c1$byClass[['Precision']])

prediction <<- prediction[-6,]
prediction <<- rbind(prediction, svm=temp1 )
prediction['svm',]


```


Can infer that:

- The cost used was 1
- Accuracy is 0.7647

### Accucuracies of all models (after testing)

```{r}

prediction

```

Can infer that:

- The accuracy of random forest is highest amongst all


### Check correlation of models(on training) before trying ensembling


```{r}

results <- resamples(list(glm = model_lr, rf = model_rf, knn = model_knn, rpart = model_rpart, nb = model_nb, svm = model_svm))
summary(results)
dotplot(results)
modelCor(results)

```



### Let's try ensembling to see if performance enhances
----------------------------------------------------------
High level steps are:

1) Train the individual base layer models on training data.
2) Predict using each base layer model for training data and test data. (One key thing to note here is that out of fold predictions are used while predicting for the training data).
3) Now train the top layer model again on the predictions of the bottom layer models that has been made on the training data.
4) Finally, predict using the top layer model with the predictions of bottom layer models that has been made for testing data.
----------------------------------------------------------
Step 1) is already done above.
----------------------------------------------------------
Let's combine different models to form an ensemble.

Two of the key principles for selecting the models:

- The individual models fulfill particular accuracy criteria.
- The model predictions of various individual models are not highly correlated with the predictions of other models.

Let's try Random forest and knn (as correlation is least)
-----------------------------------------------------------

### Step 2) Predict using each base layer model for training data and test data.

```{r}

#Predicting the out of fold prediction probabilities for training data

trainSet1$OOF_pred_rf<-model_rf$pred$Y[order(model_rf$pred$rowIndex)]
trainSet1$OOF_pred_knn<-model_knn$pred$Y[order(model_knn$pred$rowIndex)]



#Predicting probabilities for the test data

testSet1$OOF_pred_rf<-predict(model_rf,testSet1[predictors],type='prob')$Y
testSet1$OOF_pred_knn<-predict(model_knn,testSet1[predictors],type='prob')$Y


```


### step 3) Now train the top layer model again on the predictions of the bottom layer models that has been made on the training data.

```{r}

#Predictors for top layer models 
predictors_top <- c('OOF_pred_rf','OOF_pred_knn') 

set.seed(101)

#GBM as top layer model (Gradient Boosting)
model_gbm <- train(trainSet1[,predictors_top],trainSet1[,outcomeName],method='gbm',trControl=fitControl,verbose = FALSE)
#verbose: logical. Should R report extra information on progress? If TRUE then report progress
model_gbm

```


### Step 4: Finally, predict using the top layer model with the predictions of bottom layer models that has been made for testing data


```{r}
#predict using GBM top layer model
testSet1$gbm_stacked<-predict(model_gbm,testSet1[,predictors_top])

```

```{r}
#check the accuracy
confusionMatrix(testSet1$gbm_stacked,testSet1$Loan_Status,positive = 'Y')
```

Can infer that:

- The models are quite similar in nature. That is the reason ensembling is having no effect.
- The best model is random forest. Let's try tuning it further

```{r}

#Training the random forest model

set.seed(101)
#using 10-fold cross validation

model_rf_tune<-train(trainSet1[,predictors],trainSet1[,outcomeName],method='rf',trControl=fitControl,nodesize=2,ntree=1000)
model_rf_tune

#Predicting using random forest model
predict_model_rf_tune<-predict(model_rf_tune,testSet1[,predictors])

#Checking the accuracy of the random forest model
confusionMatrix(predict_model_rf_tune,testSet1[,outcomeName],positive = 'Y')


```

- The accuracy improved very slightly only (from 0.7843 to 0.7908)
- Sensitivity also increased


### Credit History as main variable


```{r}

LoanStat <- ifelse(loan_train1$Loan_Status=='Y',1,0)
table(Loan_Status=LoanStat)
table(Credit_History=loan_train1$Credit_History)
confusionMatrix(loan_train1$Credit_History,LoanStat)

```

Can infer that:

- If we simply say credit history of 1(good), approve the loan and credit history of 0(not good) then don't approve the loan, the accuracy of such a inference is 0.7687 which is quite good








